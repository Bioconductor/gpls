---
title: "Classification using Generalized Partial Least Squares"
author:
- name: "Beiying Ding"
- name: "Robert Gentleman"
- name: "Paul Villafuerte"
  affiliation: "Vignette translation from Sweave to Rmarkdown / HTML"
date: "`r format(Sys.time(), '%B %d, %Y')`"
vignette: >
  %\VignetteIndexEntry{gpls Tutorial}
  %\VignettePackage{gpls}
  %\VignetteIndexEntry{Quick-start Guide}
  %\VignetteEncoding{UTF-8}
bibliography:
- gpls.bib
output:
  BiocStyle::html_document:
    number_sections: true
    toc: false
    toc_depth: 4
---

```{r message=FALSE, warning=FALSE, include=FALSE}
BiocManager::install('gpls')
library(gpls) 
```

# Introduction {#introduction .unnumbered}

The `r Biocpkg('gpls')` package includes functions for classification using
generalized partial least squares approaches. Both two-group and
multi-group (more than 2 groups) classifications can be done. The basic
functionalities are based on and extended from the Iteratively
ReWeighted Least Squares (IRWPLS) by @marx:1996. Additionally, Firth's
bias reduction procedure [@firth:1992:a; @firth:1992:b; @firth:1993] is
incorporated to remedy the nonconvergence problem frequently encountered
in logistic regression. For more detailed description of classification
using generalized partial least squares, refer to @ding:2003:c.

# The `glpls1a` function {#the-glpls1a-function .unnumbered}

The `glpls1a` function carries out two-group classification via
IRWPLS(F). Whether or not to use Firth's bias reduction is an option
(`br=T`). The X matrix shouldn't include an intercept term. 

```{r message=FALSE}
library(gpls)
```

```{r}
set.seed(123)
x <- matrix(rnorm(20),ncol=2)
y <- sample(0:1,10,TRUE)
## no bias reduction
glpls1a(x,y,br=FALSE)

## no bias reduction and 1 PLS component
glpls1a(x,y,K.prov=1,br=FALSE)

## bias reduction
glpls1a(x,y,br=TRUE)
```

`K.prov` specifies the number PLS components to use. Note that when
`K.prov` is no specified, the number of PLS components are set to be the
smaller of the row and column rank of the design matrix.

# The `glpls1a.cv.error` and `glpls1a.train.test.error` functions {#the-glpls1a.cv.error-and-glpls1a.train.test.error-functions .unnumbered}

The `glpls1a.cv.error` calculates leave-one-out classification error
rate for two-group classification and `glpls1a.train.test.error`
calculates test set error where the model is fit using the training
set.

```{r}
## training set
x <- matrix(rnorm(20),ncol=2)
y <- sample(0:1,10,TRUE)
## test set
x1 <- matrix(rnorm(10),ncol=2)
y1 <- sample(0:1,5,TRUE)
## no bias reduction
glpls1a.cv.error(x,y,br=FALSE)
glpls1a.train.test.error(x,y,x1,y1,br=FALSE)
## bias reduction and 1 PLS component
glpls1a.cv.error(x,y,K.prov=1,br=TRUE)
glpls1a.train.test.error(x,y,x1,y1,K.prov=1,br=TRUE)
```

# The `glpls1a.mlogit` and `glpls1a.logit.all` functions {#the-glpls1a.mlogit-and-glpls1a.logit.all-functions .unnumbered}

The `glpls1a.mlogit` carries out multi-group classification using
MIRWPLS(F) where the baseline logit model is used as counterpart to
`glpls1a` for two group case. `glpls1a.logit.all` carries out
multi-group classification by separately fitting $C$ two-group
classification using `glpls1a` separately for $C$ group vs the same
baseline class (i.e. altogether $C+1$ classes). This separate fitting of
logit is known to be less efficient but has been used in practice due to
its more straightforward implementation.

Note that when using `glpls1a.mlogit`, the X matrix needs to have a
column of one, i.e. intercept term.

```{r}
x <- matrix(rnorm(20),ncol=2)
y <- sample(1:3,10,TRUE)
## no bias reduction and 1 PLS component
glpls1a.mlogit(cbind(rep(1,10),x),y,K.prov=1,br=FALSE)
glpls1a.logit.all(x,y,K.prov=1,br=FALSE)
## bias reduction
glpls1a.mlogit(cbind(rep(1,10),x),y,br=TRUE)
glpls1a.logit.all(x,y,br=TRUE)
```

# The `glpls1a.mlogit.cv.error` {#the-glpls1a.mlogit.cv.error-function .unnumbered}

The `glpls1a.mlogit.cv.error` calculates leave-one-out error for
multi-group classification using (M)IRWPLS(F). When the `mlogit` option
is set to be true, then `glpls1a.mlogit` is used, else
`glpls1a.logit.all` is used for fitting.
 
```{r}
x <- matrix(rnorm(20),ncol=2)
y <- sample(1:3,10,TRUE)
## no bias reduction and 1 PLS component
glpls1a.mlogit(cbind(rep(1,10),x),y,K.prov=1,br=FALSE)
glpls1a.logit.all(x,y,K.prov=1,br=FALSE)
## bias reduction
glpls1a.mlogit(cbind(rep(1,10),x),y,br=TRUE)
glpls1a.logit.all(x,y,br=TRUE)
```

## Fitting Models to data

Here we demonstrate the use of `r Biocpkg('gpls')` on some standard machine learning examples. We first make use of the Pima Indian data from the MASS
package.

```{r pimaEx}
library(MASS)
 m1 = gpls(type~., Pima.tr)
 p1 = predict(m1, Pima.te[,-8])
##when we get to the multi-response problems
     data(iris3)
     Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
                        Sp = rep(c("s","c","v"), rep(50,3)))
     train <- sample(1:150, 75)
     table(Iris$Sp[train])
     ## your answer may differ
     ##  c  s  v
     ## 22 23 30
     z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = train)
     predict(z, Iris[-train, ])$class
```
